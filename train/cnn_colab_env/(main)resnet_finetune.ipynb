{"cells":[{"cell_type":"markdown","metadata":{"id":"rgxis8PO3oWK"},"source":["# README"]},{"cell_type":"markdown","metadata":{"id":"bV3flEj3_8GU"},"source":["### info\n","\n","1. task\n","  - croped meat object image doneness classification\n","  - input : color image , output : N class classification (0~N-1)\n","2. base model\n","  - pretrained ResNet50 with Imagenet"]},{"cell_type":"markdown","metadata":{"id":"kSZmt9rTw4Fc"},"source":["### 학습 시 변경해볼 사항\n","\n","1. **hyperparameter**\n","  - batchsize\n","  - epoch\n","  - (learning rate)\n","\n","2. **freezing layer 조절**\n","  - classifier + 상단 layer\n","\n","3. data augmentation 조절 (aug 많이 할수록 epoch도 늘리기)\n","  - RandomResizedCrop\n","  - RandomVerticalFlip/RandomHorizontalFlip\n","  - RandomRotation\n","  - ColorJitter\n","  - GaussianBlur\n","  - (+ RandomAffine, RandomGrayscale, RandomPerspective, ..)\n"]},{"cell_type":"markdown","metadata":{"id":"-dwywp-e2JgS"},"source":["### 커스텀 데이터셋 폴더구조\n","\n","  ```\n","  dataset/\n","  ├── train/\n","  │   ├── class1/\n","  │   ├── class2/\n","  │   └── class3/\n","  ├── val/\n","  │   ├── class1/\n","  │   ├── class2/\n","  │   └── class3/\n","  └── test/\n","      ├── class1/\n","      ├── class2/\n","      └── class3/\n","  ```\n"]},{"cell_type":"markdown","metadata":{"id":"4FoeVylz3mtp"},"source":["# CODE"]},{"cell_type":"markdown","metadata":{"id":"qdoI_qzeZckb"},"source":["### dataset utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzv2RC6hZWeJ"},"outputs":[],"source":["import os\n","import glob\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7krrylpZotL"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, root_dir, data_type, transform=None):\n","        \"\"\"\n","        Initializes the dataset.\n","\n","        :param root_dir: The root directory where the data is stored.\n","        :param data_type: The type of data to load ('train', 'val', or 'test').\n","        :param transform: The transformations to be applied to the images.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.data_type = data_type\n","        self.data = []\n","        self.labels = []\n","\n","        dataset_path = os.path.join(self.root_dir, data_type)\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(os.listdir(dataset_path))}\n","        for cls_name, idx in self.class_to_idx.items():\n","            cls_dir = os.path.join(dataset_path, cls_name)\n","            for img_name in os.listdir(cls_dir):\n","                if img_name.lower().endswith(('png', 'jpg', 'jpeg')):\n","                    self.data.append(os.path.join(cls_dir, img_name))\n","                    self.labels.append(idx)\n","\n","        print(\"------------------------------------\")\n","        print(f\"Dataset Type: {data_type}\")\n","        print(f\"Class Index Mapping: {self.class_to_idx}\")\n","        print(f\"Number of Images: {len(self.data)}\")\n","        print(\"------------------------------------\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuexuBhhZvV8"},"outputs":[],"source":["transform = {\n","    'train': transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Resize((224, 224)), # ResNet input size\n","                                # transforms.Normalize([meanR, meanG, meanB], [stdR, stdG, stdB]) normalize the color value of IMAGENET\n","                                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","                                # augmentations\n","                                transforms.RandomResizedCrop((224, 224)), # 무작위 크롭 후 224x224로 리사이즈\n","                                transforms.RandomVerticalFlip(),   # 50% 확률로 수직 뒤집기\n","                                transforms.RandomHorizontalFlip(), # 50% 확률로 수평 뒤집기\n","                                transforms.RandomRotation(10),     # -10도에서 10도 사이로 무작위 회전\n","                                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 색상 변조\n","                                transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),  # 가우시안 블러 적용\n","                                ]),\n","    'eval': transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Resize((224, 224)),\n","                                # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","                              ])\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0nm7T2gZyl8"},"outputs":[],"source":["def show_dataloader(dataloader, title, num_images=4):\n","    TOTAL_WIDTH = 8 # 10\n","    plt.figure(figsize=(TOTAL_WIDTH, 1+TOTAL_WIDTH/num_images))\n","    col_num = num_images\n","    data_exists = False\n","\n","    for batch_idx, (images, labels) in enumerate(dataloader):\n","        data_exists = True\n","        if batch_idx == 0:\n","            for i in range(min(num_images, len(images))):\n","                plt.subplot(1, col_num, i+1)\n","                plt.axis('off')\n","                image_np = images[i].numpy()\n","                image_np = np.transpose(image_np, (1, 2, 0))\n","                plt.imshow(image_np)\n","                plt.title(str(labels[i].item()))\n","            plt.suptitle(title)\n","            break\n","\n","    if not data_exists:\n","        plt.text(0.5, 0.5, 'No data available', ha='center', va='center', fontsize=12)\n","        plt.suptitle(title)"]},{"cell_type":"markdown","metadata":{"id":"ZcCqCGYMZd3z"},"source":["### log utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jr4IXl3nZ1cH"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4P0BhyoZeNb"},"outputs":[],"source":["def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPwnIKGgZ8FL"},"outputs":[],"source":["def result_plot(history):\n","    acc = history['train_acc_arr']\n","    val_acc = history['val_acc_arr']\n","    loss = history['train_loss_arr']\n","    val_loss = history['val_loss_arr']\n","\n","    plt.clf()\n","    plt.figure(figsize=(15, 5))\n","\n","    epochs = range(1, len(acc) + 1)\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, acc, 'bo', label='Training acc')\n","    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, loss, 'bo', label='Training loss')\n","    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"eu6qCm5XZecj"},"source":["### train setting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJAx_ZjbaB5_"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.models as models\n","from torchvision.models import ResNet50_Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16900,"status":"ok","timestamp":1715208623303,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"3wm6fs5bahG3","outputId":"44da52eb-68ee-4af4-f8f7-43cbd77c61ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ygo7XIgaIb4"},"outputs":[],"source":["# dataset\n","DATA_PATH = '/content/drive/MyDrive/dataset'\n","NUM_CLASSES = 2\n","# hyperparameter\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.001\n","NUM_EPOCHS = 2\n","# model save\n","MODEL_PATH = '/content/drive/MyDrive/weights/resnet_finetuned.pth'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmPyi2KkaI0n"},"outputs":[],"source":["def load_pretrained_resnet(class_num):\n","    resnet50 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)  # 혹은 models.resnet50(pretrained=True)\n","    # change classifier\n","    resnet50.fc = nn.Linear(in_features=2048, out_features=class_num, bias=True)\n","    # freeze classifier\n","    for name, p in resnet50.named_parameters():\n","        if 'fc' in name:\n","            p.requires_grad = True\n","        else:\n","            p.requires_grad = False\n","    return resnet50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1NxTHY0aKUH"},"outputs":[],"source":["def check_params_to_update(model):\n","    params_to_update = []\n","    for name, param in model.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\", name)\n","    return params_to_update"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWZo02X2Z50U"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=2):\n","    print(\"Start Training\")\n","    history = {\n","        'train_loss_arr' : [],\n","        'train_acc_arr' : [],\n","        'val_loss_arr' : [],\n","        'val_acc_arr' : [],\n","    }\n","\n","\n","    for epoch in range(num_epochs):\n","        print(f\"=====================================\")\n","        print(f\"Epoch: {epoch + 1}/{num_epochs} \")\n","\n","        # Training phase\n","        model.train()\n","        train_losses = AverageMeter()\n","        train_top1 = AverageMeter()\n","\n","        for i, (data, target) in enumerate(train_loader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            prec1 = accuracy(output.data, target)[0]\n","\n","            train_losses.update(loss.item(), data.size(0))\n","            train_top1.update(prec1.item(), data.size(0))\n","\n","            # 매 batch 마다 결과 출력\n","            print(f'Train Batch: [{i}/{len(train_loader)}]\\t'\n","                  f'Loss {train_losses.val:.4f} ({train_losses.avg:.4f})')\n","\n","        history['train_loss_arr'].append(train_losses.avg)\n","        history['train_acc_arr'].append(train_top1.avg)\n","\n","        print(f\"Train result: Loss: {train_losses.avg}, Acc: {train_top1.avg}\\n\")\n","\n","        # Validation phase\n","        model.eval()\n","        val_losses = AverageMeter()\n","        val_top1 = AverageMeter()\n","\n","        with torch.no_grad():\n","            for i, (data, target) in enumerate(val_loader):\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                loss = criterion(output, target)\n","\n","                prec1 = accuracy(output.data, target)[0]\n","\n","                val_losses.update(loss.item(), data.size(0))\n","                val_top1.update(prec1.item(), data.size(0))\n","\n","                # 매 batch 마다 결과 출력\n","                print(f'Val Batch: [{i}/{len(val_loader)}]\\t'\n","                      f'Loss {val_losses.val:.4f} ({val_losses.avg:.4f})\\t'\n","                      f'Prec@1 {val_top1.val:.3f} ({val_top1.avg:.3f})')\n","\n","            history['val_loss_arr'].append(val_losses.avg)\n","            history['val_acc_arr'].append(val_top1.avg)\n","\n","            print(f\"Validation result: Loss: {val_losses.avg}, Acc: {val_top1.avg}\\n\")\n","\n","        print(f\"=====================================\")\n","\n","    print('Finished Training')\n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfQ2C2v8bZdS"},"outputs":[],"source":["def evaluate_model(model, test_loader, criterion, device):\n","    print(f\"=====================================\")\n","    model.eval()\n","    test_losses = AverageMeter()\n","    test_accuracy = AverageMeter()\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion(output, target)\n","            prec1 = accuracy(output, target)[0]\n","\n","            test_losses.update(loss.item(), data.size(0))\n","            test_accuracy.update(prec1.item(), data.size(0))\n","\n","    print(f\"Test Result - Loss: {test_losses.avg:.4f}, Accuracy: {test_accuracy.avg:.2f}%\")\n","    print(f\"=====================================\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3123,"status":"ok","timestamp":1715208636046,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"srMK0EzjaUiY","outputId":"eedaa116-67f3-4278-d90f-ef5c9f4dd8f6"},"outputs":[],"source":["# load data\n","train_dataset = CustomDataset(DATA_PATH, data_type='train', transform=transform['train'])\n","val_dataset = CustomDataset(DATA_PATH, data_type='val', transform=transform['eval'])\n","test_dataset = CustomDataset(DATA_PATH, data_type='test', transform=transform['eval'])\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":873},"executionInfo":{"elapsed":79383,"status":"ok","timestamp":1715210524404,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"gkP0PVDFaWET","outputId":"d0b68791-ac8f-43cb-8307-8d96883cfe45"},"outputs":[],"source":["# show few part of train & val dataset\n","show_dataloader(train_loader, title=\"train dataset\")\n","show_dataloader(val_loader, title=\"val dataset\")\n","show_dataloader(test_loader, title=\"test dataset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1774,"status":"ok","timestamp":1715209749605,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"9q4ZBnN3mUXE","outputId":"4d4b7edf-4d13-4888-b542-9541a77f2d98"},"outputs":[],"source":["# train setting essential\n","GPU_NUM = 0\n","device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n","model = load_pretrained_resnet(NUM_CLASSES).to(device)\n","criterion = nn.CrossEntropyLoss()\n","params_to_update = check_params_to_update(model)\n","optimizer_ft = torch.optim.SGD(\n","    params_to_update, lr=LEARNING_RATE, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1715209756482,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"SRUMHIb5mac8","outputId":"63eee7ca-4eaf-49fd-e3a0-26283ad767a4"},"outputs":[],"source":["print(model) # check code\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"7uOGuSevogoN"},"source":["### main"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"elapsed":9920,"status":"error","timestamp":1715210534321,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"F9wjESUxmVl6","outputId":"bc15d64d-6d73-42b0-b4da-acd1eae41dc8"},"outputs":[],"source":["# train\n","fine_model, history = train_model(model, train_loader, val_loader, criterion, optimizer_ft, device, num_epochs=NUM_EPOCHS)\n","\n","# save\n","torch.save(fine_model.state_dict(), MODEL_PATH)\n","# ckpts = torch.load(MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":1255,"status":"ok","timestamp":1715210537291,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"-eXQ2udoZeqL","outputId":"98392b35-8c94-4619-eb4e-fdd09be5fd7f"},"outputs":[],"source":["# show\n","result_plot(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29675,"status":"ok","timestamp":1715210570063,"user":{"displayName":"gogi_deep","userId":"07733643897937585097"},"user_tz":-540},"id":"52RySFWkbQXz","outputId":"4d126923-c0e8-4895-aff6-9f5edb230b12"},"outputs":[],"source":["# evaluate with test set\n","evaluate_model(model, test_loader, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b16uoW8_f7Ua"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPvnfdcnvtDiAzVSeqKZGif","gpuType":"T4","mount_file_id":"1fSlzVl7mVXv240CTnEYuPDue5v31Lrwk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
